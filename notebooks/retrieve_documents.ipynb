{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import NotionDBLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotionSemanticSearch:\n",
    "    def __init__(self, integration_token, database_id, model_name='all-MiniLM-L6-v2', embedding_dir='./embeddings'):\n",
    "        \"\"\"\n",
    "        Initialize the NotionSemanticSearch class.\n",
    "        \"\"\"\n",
    "        self.integration_token = integration_token\n",
    "        self.database_id = database_id\n",
    "        self.loader = NotionDBLoader(\n",
    "            integration_token=self.integration_token,\n",
    "            database_id=self.database_id\n",
    "        )\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.embedded_chunks = []\n",
    "        self.chunked_documents = []\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"\n",
    "        Load documents from the Notion database.\n",
    "        \"\"\"\n",
    "        documents = self.loader.load()\n",
    "        print(f\"Loaded {len(documents)} documents from Notion.\")\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents, chunk_size=300, chunk_overlap=50):\n",
    "        \"\"\"\n",
    "        Split documents into smaller chunks for embedding.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \".\", \" \"]\n",
    "        )\n",
    "        chunked_documents = []\n",
    "        for doc in documents:\n",
    "            chunks = text_splitter.split_text(doc.page_content)\n",
    "            for chunk in chunks:\n",
    "                chunked_documents.append({\n",
    "                    \"content\": chunk,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "        print(f\"Split into {len(chunked_documents)} chunks.\")\n",
    "        self.chunked_documents = chunked_documents\n",
    "        return chunked_documents\n",
    "\n",
    "    def generate_local_embeddings(self):\n",
    "        \"\"\"\n",
    "        Generate embeddings for document chunks.\n",
    "        \"\"\"\n",
    "        embedded_chunks = []\n",
    "        for chunk in self.chunked_documents:\n",
    "            embedding = self.model.encode(chunk['content'])\n",
    "            embedded_chunks.append({'embedding': embedding, 'metadata': chunk['metadata']})\n",
    "        print(f\"Generated embeddings for {len(embedded_chunks)} chunks.\")\n",
    "        self.embedded_chunks = embedded_chunks\n",
    "        return embedded_chunks\n",
    "\n",
    "    def save_embeddings(self, filename='embedded_chunks.pkl'):\n",
    "        \"\"\"\n",
    "        Save the embeddings to a file.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.embedding_dir, exist_ok=True)\n",
    "        filepath = os.path.join(self.embedding_dir, filename)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.embedded_chunks, f)\n",
    "        print(f\"Saved embeddings to {filepath}.\")\n",
    "\n",
    "    def load_embeddings(self, filename='embedded_chunks.pkl'):\n",
    "        \"\"\"\n",
    "        Load embeddings from a file.\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.embedding_dir, filename)\n",
    "        with open(filepath, 'rb') as f:\n",
    "            self.embedded_chunks = pickle.load(f)\n",
    "        print(f\"Loaded embeddings from {filepath}.\")\n",
    "        return self.embedded_chunks\n",
    "\n",
    "    def search_documents(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Search for relevant documents based on a query.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(query).reshape(1, -1)\n",
    "        embeddings = np.array([chunk['embedding'] for chunk in self.embedded_chunks])\n",
    "        similarities = np.dot(embeddings, query_embedding.T).flatten()\n",
    "        ranked_indices = similarities.argsort()[::-1][:top_k]\n",
    "        results = [(self.embedded_chunks[i], similarities[i]) for i in ranked_indices]\n",
    "        return results\n",
    "\n",
    "    def fetch_full_document(self, doc_id):\n",
    "        \"\"\"\n",
    "        Fetch the full document details by its ID.\n",
    "        \"\"\"\n",
    "        documents = self.loader.load()\n",
    "        for document in documents:\n",
    "            if document.metadata.get(\"id\") == doc_id:\n",
    "                return {\n",
    "                    \"title\": document.metadata.get(\"title\", \"No Title\"),\n",
    "                    \"content\": document.page_content\n",
    "                }\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 documents from Notion.\n",
      "Split into 14 chunks.\n",
      "Generated embeddings for 14 chunks.\n",
      "Saved embeddings to ./embeddings/embedded_chunks.pkl.\n"
     ]
    }
   ],
   "source": [
    "notion_search = NotionSemanticSearch(\n",
    "    integration_token=os.getenv(\"NOTION_INTEGRATION_TOKEN\"),\n",
    "    database_id=os.getenv(\"NOTION_DATABASE_ID\")\n",
    ")\n",
    "\n",
    "documents = notion_search.load_documents()\n",
    "chunked_documents = notion_search.split_documents(documents)\n",
    "\n",
    "notion_search.generate_local_embeddings()\n",
    "\n",
    "notion_search.save_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from ./embeddings/embedded_chunks.pkl.\n",
      "Result 1:\n",
      "Score: 0.5359396934509277\n",
      "Content: {'details': 'Guidelines for remote work, including eligibility, expectations, and communication protocols.', 'category': ['HR'], 'policy name': 'Remote Work Policy', 'id': '147d4f32-0aa2-80c9-8ede-fb4b96209c97'}\n",
      "Result 2:\n",
      "Score: 0.3378417491912842\n",
      "Content: {'details': 'Measures to protect company data, employee responsibilities, and procedures for reporting security incidents.', 'category': ['IT'], 'policy name': 'Data Security Policy', 'id': '147d4f32-0aa2-80f6-945d-eb9cd9434c0c'}\n",
      "Result 3:\n",
      "Score: 0.3314809799194336\n",
      "Content: {'details': 'Rules regarding the use of company IT resources, including internet usage, software installations, and personal device policies.', 'category': ['IT'], 'policy name': 'IT Acceptable Use Policy', 'id': '147d4f32-0aa2-805d-980f-f799e21b2800'}\n"
     ]
    }
   ],
   "source": [
    "notion_search.load_embeddings()\n",
    "\n",
    "query = \"What are the guidelines for remote work?\"\n",
    "results = notion_search.search_documents(query, top_k=3)\n",
    "\n",
    "for i, (result, score) in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Content: {result['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: At NextGen Enterprises, employees can work remotely for up to three days a week. Eligible roles require manager approval and a signed Remote Work Agreement. Daily stand-up meetings are conducted via Zoom, and employees must update project status on the internal tracker.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_id = results[0][0]['metadata']['id'] \n",
    "full_document = notion_search.fetch_full_document(document_id)\n",
    "if full_document:\n",
    "    print(f\"Content: {full_document['content']}\")\n",
    "else:\n",
    "    print(\"Document not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaSemanticSearch:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', chroma_dir='./chroma_data'):\n",
    "        \"\"\"\n",
    "        Initialize the ChromaSemanticSearch class.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.chroma_dir = chroma_dir\n",
    "        self.vectorstore = None\n",
    "        self.embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    def create_vectorstore(self, chunked_documents):\n",
    "        \"\"\"\n",
    "        Create a Chroma vectorstore and store document embeddings.\n",
    "\n",
    "        Args:\n",
    "            chunked_documents (list): List of dictionaries with `content` and `metadata`.\n",
    "        \"\"\"\n",
    "        texts = [chunk[\"content\"] for chunk in chunked_documents]\n",
    "        metadatas = [chunk[\"metadata\"] for chunk in chunked_documents]\n",
    "\n",
    "        # Create and persist the Chroma vectorstore\n",
    "        self.vectorstore = Chroma.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=self.embedding_function,\n",
    "            metadatas=metadatas,\n",
    "            persist_directory=self.chroma_dir,\n",
    "        )\n",
    "        self.vectorstore.persist()\n",
    "        print(\"Created and persisted the Chroma vectorstore.\")\n",
    "\n",
    "    def load_vectorstore(self):\n",
    "        \"\"\"\n",
    "        Load an existing Chroma vectorstore.\n",
    "        \"\"\"\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=self.chroma_dir,\n",
    "            embedding_function=self.embedding_function,\n",
    "        )\n",
    "        print(\"Loaded the Chroma vectorstore.\")\n",
    "\n",
    "    def search_documents(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Search for relevant documents in the Chroma vectorstore based on a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "            top_k (int): The number of top results to return.\n",
    "\n",
    "        Returns:\n",
    "            list: List of search results with metadata.\n",
    "        \"\"\"\n",
    "        # Ensure the vectorstore is loaded\n",
    "        if not self.vectorstore:\n",
    "            self.load_vectorstore()\n",
    "\n",
    "        results = self.vectorstore.similarity_search(query, k=top_k)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metadata(chunked_documents):\n",
    "    \"\"\"\n",
    "    Preprocess metadata to ensure all values are of valid types for Chroma.\n",
    "    \n",
    "    Args:\n",
    "        chunked_documents (list): List of dictionaries with `content` and `metadata`.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list of documents with sanitized metadata.\n",
    "    \"\"\"\n",
    "    sanitized_documents = []\n",
    "    for chunk in chunked_documents:\n",
    "        sanitized_metadata = {\n",
    "            key: (value[0] if isinstance(value, list) and len(value) > 0 else value)\n",
    "            if isinstance(value, list) else value\n",
    "            for key, value in chunk[\"metadata\"].items()\n",
    "        }\n",
    "        sanitized_documents.append({\n",
    "            \"content\": chunk[\"content\"],\n",
    "            \"metadata\": sanitized_metadata\n",
    "        })\n",
    "    return sanitized_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/f0l_v3gs35b3lwyrxkgg9btr0000gp/T/ipykernel_4095/3665221142.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and persisted the Chroma vectorstore.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/f0l_v3gs35b3lwyrxkgg9btr0000gp/T/ipykernel_4095/3665221142.py:28: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Preprocess Metadata\n",
    "sanitized_chunked_documents = preprocess_metadata(chunked_documents)\n",
    "\n",
    "# Step 5: Create Chroma vectorstore\n",
    "chroma_search = ChromaSemanticSearch()\n",
    "chroma_search.create_vectorstore(sanitized_chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: At NextGen Enterprises, employees can work remotely for up to three days a week. Eligible roles require manager approval and a signed Remote Work Agreement. Daily stand-up meetings are conducted via Zoom, and employees must update project status on the internal tracker.\n",
      "Metadata: {'category': 'HR', 'details': 'Guidelines for remote work, including eligibility, expectations, and communication protocols.', 'id': '147d4f32-0aa2-80c9-8ede-fb4b96209c97', 'policy name': 'Remote Work Policy'}\n",
      "Result 2:\n",
      "Content: Employees at NextGen Enterprises are encouraged to promote company achievements but must avoid sharing confidential projects. Approved templates for LinkedIn and Twitter posts are available in the \"Employee Toolkit.\" Social media training is mandatory for marketing teams.\n",
      "Metadata: {'category': 'Marketing', 'details': 'Best practices for employees when representing the company on social media platforms, including confidentiality and brand representation.', 'id': '147d4f32-0aa2-80ab-969b-eae27196c28c', 'policy name': 'Social Media Usage Guidelines'}\n",
      "Result 3:\n",
      "Content: All employees of NextGen Enterprises must complete mandatory cybersecurity training annually. The policy includes password requirements, guidelines for using VPNs, and procedures for reporting phishing emails to the IT Security Team through the \"Report an Incident\" portal.\n",
      "Metadata: {'category': 'IT', 'details': 'Measures to protect company data, employee responsibilities, and procedures for reporting security incidents.', 'id': '147d4f32-0aa2-80f6-945d-eb9cd9434c0c', 'policy name': 'Data Security Policy'}\n"
     ]
    }
   ],
   "source": [
    "chroma_search = ChromaSemanticSearch()\n",
    "\n",
    "query = \"I want to work from home, what should I know about it?\"\n",
    "results = chroma_search.search_documents(query, top_k=3)\n",
    "\n",
    "# Step 7: Display results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
