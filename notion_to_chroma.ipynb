{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import NotionDBLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\n",
    "database_id = os.getenv(\"NOTION_DATABASE_ID\")\n",
    "chroma_dir = './chroma_data'\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "collection_name = \"documents_collection\"\n",
    "\n",
    "if not integration_token or not database_id:\n",
    "    raise ValueError(\"Missing Notion API token or Database ID in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cmg203/Desktop/notion-bot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/1v/f0l_v3gs35b3lwyrxkgg9btr0000gp/T/ipykernel_33235/2511591070.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/Users/cmg203/Desktop/notion-bot/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from Notion...\n",
      "Loaded 14 documents from Notion.\n",
      "Splitting documents into smaller chunks...\n",
      "Split into 14 chunks.\n",
      "Sanitizing metadata...\n",
      "Creating Chroma vectorstore...\n",
      "Chroma vectorstore created and persisted successfully.\n",
      "Process completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/f0l_v3gs35b3lwyrxkgg9btr0000gp/T/ipykernel_33235/2511591070.py:80: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Initialize loaders and models\n",
    "loader = NotionDBLoader(integration_token=integration_token, database_id=database_id)\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load documents from Notion\n",
    "print(\"Loading documents from Notion...\")\n",
    "documents = loader.load()\n",
    "if not documents:\n",
    "    raise ValueError(\"No documents were loaded from Notion.\")\n",
    "print(f\"Loaded {len(documents)} documents from Notion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split documents into chunks\n",
    "print(\"Splitting documents into smaller chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50, separators=[\"\\n\\n\", \".\", \" \"])\n",
    "chunked_documents = [\n",
    "    {\"content\": chunk, \"metadata\": doc.metadata}\n",
    "    for doc in documents\n",
    "    for chunk in text_splitter.split_text(doc.page_content)\n",
    "]\n",
    "if not chunked_documents:\n",
    "    raise ValueError(\"No chunks were created from the documents.\")\n",
    "print(f\"Split into {len(chunked_documents)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess metadata\n",
    "def preprocess_metadata(chunked_documents):\n",
    "    sanitized_documents = []\n",
    "    for chunk in chunked_documents:\n",
    "        sanitized_metadata = {\n",
    "            key: (value[0] if isinstance(value, list) and len(value) > 0 else value)\n",
    "            if isinstance(value, list) else value\n",
    "            for key, value in chunk[\"metadata\"].items()\n",
    "        }\n",
    "        sanitized_documents.append({\n",
    "            \"content\": chunk[\"content\"],\n",
    "            \"metadata\": sanitized_metadata\n",
    "        })\n",
    "    return sanitized_documents\n",
    "\n",
    "print(\"Sanitizing metadata...\")\n",
    "sanitized_documents = preprocess_metadata(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and persist Chroma vectorstore\n",
    "print(\"Creating Chroma vectorstore...\")\n",
    "texts = [doc[\"content\"] for doc in sanitized_documents]\n",
    "metadatas = [doc[\"metadata\"] for doc in sanitized_documents]\n",
    "\n",
    "try:\n",
    "    vectorstore = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_function,\n",
    "        metadatas=metadatas,\n",
    "        persist_directory=chroma_dir,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(\"Chroma vectorstore created and persisted successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error creating Chroma vectorstore: {e}\")\n",
    "\n",
    "print(\"Process completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
